# ğŸš€ Cliffracer Load Testing Suite

Comprehensive performance testing and benchmarking for Cliffracer using Locust.

## ğŸ“Š What We Test

### **Performance Scenarios**
- **High-throughput RPC calls** with complex object validation
- **Error handling resilience** - RPS maintained during failures
- **Concurrent service communication** patterns
- **Memory and latency under sustained load**

### **Comparison Benchmarks**  
- **Cliffracer vs FastAPI + Redis**
- **Cliffracer vs Flask + RabbitMQ**
- **NATS vs HTTP/REST performance**

### **Complex Data Validation**
- **Nested object hierarchies** (orders with items, addresses, payments)
- **Large payload processing** (file uploads, batch operations)
- **Schema validation overhead** measurement
- **Serialization performance** testing

## ğŸ¯ Test Scenarios

### **1. E-commerce Load Test**
- Order processing with complex validation
- Payment processing with retry logic
- Inventory updates with concurrency
- Error injection (payment failures, validation errors)

### **2. Real-time Analytics**
- High-frequency event ingestion
- Complex aggregation operations
- Time-series data processing
- Concurrent read/write patterns

### **3. File Processing**
- Large file upload simulation
- Batch processing workflows
- Multi-step validation pipelines
- Error recovery testing

## ğŸƒâ€â™‚ï¸ Quick Start

```bash
# Install dependencies
pip install locust

# Start test services
./scripts/start-test-services.sh

# Run basic load test
locust -f tests/basic_load_test.py --host=nats://localhost:4222

# Run comparison benchmark
./scripts/run-benchmarks.sh

# Generate performance reports
./scripts/generate-reports.sh
```

## ğŸ“ Directory Structure

```
load-testing/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ test-config.yaml
â”‚   â””â”€â”€ benchmark-config.yaml
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ cliffracer-services/    # Test services using Cliffracer
â”‚   â”œâ”€â”€ fastapi-services/       # Comparison services using FastAPI
â”‚   â””â”€â”€ shared/                 # Shared data models and utilities
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ locust/                 # Locust test files
â”‚   â”œâ”€â”€ scenarios/              # Test scenario definitions
â”‚   â””â”€â”€ benchmarks/             # Comparison benchmarks
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start-test-services.sh
â”‚   â”œâ”€â”€ run-benchmarks.sh
â”‚   â””â”€â”€ generate-reports.sh
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ performance/            # Generated performance reports
â”‚   â””â”€â”€ comparisons/            # Benchmark comparison results
â””â”€â”€ data/
    â”œâ”€â”€ test-payloads/          # Sample data for testing
    â””â”€â”€ schemas/                # Complex schema definitions
```

## ğŸ“ˆ Expected Results

Based on NATS performance characteristics, we expect:

- **Sub-millisecond latency** for simple RPC calls
- **10,000+ RPS** sustained throughput  
- **Minimal latency degradation** under load
- **Consistent performance** during error conditions
- **2-5x better performance** than HTTP-based alternatives

## ğŸ”§ Test Configuration

All test parameters are configurable via YAML files:

```yaml
# config/test-config.yaml
load_test:
  users: 100
  spawn_rate: 10
  duration: 300
  
scenarios:
  ecommerce:
    order_rate: 50  # orders per second
    error_rate: 0.05  # 5% failure rate
    complex_validation: true
    
  analytics:
    event_rate: 1000  # events per second
    batch_size: 100
    concurrent_reads: 50
```